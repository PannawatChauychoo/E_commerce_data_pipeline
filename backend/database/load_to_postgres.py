import pandas as pd
import psycopg2
from psycopg2 import sql
from dotenv import load_dotenv
from pathlib import Path
import os
import sys
from sqlalchemy import create_engine


"""
Loading csv files generated by AGM into Postgres
Methods:
- Connect to the database using variables in .envs
- Create the database schema using schema.sql
- Load the csv files into the database
    - use mogrify() for efficient batch insert


"""


# Load environment variables
load_dotenv()


def connect_to_db():
    """Establish connection to PostgreSQL database."""
    try:
        conn = psycopg2.connect(
            host=str(os.getenv('DB_HOST')),
            dbname=str(os.getenv('DB_NAME')),
            user=str(os.getenv('DB_USER')),
            password=str(os.getenv('DB_PASSWORD')),
            port=str(os.getenv('DB_PORT'))
        )
        return conn
    except Exception as e:
        print(f"Error connecting to database: {e}")
        raise

def setup_database():
    """Create database schema from SQL file."""

    conn = connect_to_db()
    cur = conn.cursor()
    
    # Check if schema exists
    cur.execute("SELECT schema_name FROM information_schema.schemata WHERE schema_name = 'walmart';")
    schema_exists = cur.fetchone() is not None
    
    if not schema_exists:
        # Read and execute schema file only if schema doesn't exist
        with open('backend/database/schema.sql', 'r') as f:
            schema_sql = f.read()
        cur.execute(schema_sql)
        conn.commit()
        print("Schema 'walmart' created successfully")
    else:
        print("Schema 'walmart' already exists, skipping creation")
        
    cur.close()
    return conn  # Return the connection for further use

    
def verify_tables(tables, conn):
    """Verify tables exist and show their row counts."""
    
    cur = conn.cursor()
    
    print("\nTable Verification:")
    print("-" * 50)
    
    for table in tables:
        # Check if table exists
        cur.execute("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'walmart' 
                AND table_name = %s
            );
        """, (table,))
        exists = cur.fetchone()[0]
        
        if exists == True:
            # Get row count
            cur.execute(f"SELECT COUNT(*) FROM walmart.{table}")
            count = cur.fetchone()[0]
            print(f"✓ {table}: {count} rows")
        else:
            print(f"✗ {table}: Table not found")
    
    cur.close()

def truncate_load_csv_to_table(conn, csv_path, table_name):
    """Load data from CSV file to PostgreSQL table."""
    print(f'Loading data for {table_name}')
    
    try:
        # Read CSV file
        df = pd.read_csv(csv_path)
        
        # Convert date strings to datetime objects
        if 'date_purchased' in df.columns:
            df['date_purchased'] = pd.to_datetime(df['date_purchased'])
        
        # Get the table schema columns to ensure correct order
        cur = conn.cursor()
        cur.execute(f"TRUNCATE TABLE {table_name} CASCADE;")
        cur.execute(f"""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_schema = 'walmart' 
            AND table_name = '{table_name}'
            AND column_name NOT IN ('created_at', 'updated_at', 'transaction_id', 'customer_id')
            ORDER BY ordinal_position;
        """)
        schema_columns = [col[0] for col in cur.fetchall()]
        
        # Reorder and select only columns that exist in schema
        df.columns = df.columns.str.lower()
        df = df[schema_columns]
        
        # Dynamically setting the number of columns
        columns_str = ', '.join(schema_columns)
        placeholders_str = '(' + ', '.join(['%s'] * len(schema_columns)) + ')'  

        
        batch_size = 5000
        for i in range(0, len(df), batch_size):
            batch = df.iloc[i:i+batch_size].itertuples(index=False, name=None)
            args_str = b','.join(
                cur.mogrify(placeholders_str, row) for row in batch).decode('utf-8')
            
            # Add schema prefix to table name
            insert_query = f"INSERT INTO {table_name} ({columns_str}) VALUES {args_str}"
            cur.execute(insert_query)
            conn.commit()
        
        cur.close()
        print(f"Successfully loaded data to {table_name}")
    except Exception as e:
        print(f"Failed to load data to {table_name}: {e}")
        raise
    
def load_customer_lookup(conn):
    """Create a lookup table for customer IDs for transaction table PK."""
    cur = conn.cursor()
    
    cur.execute("TRUNCATE TABLE customers CASCADE;")
    
    #Populate customers lookup table with data from cust1 and cust2
    cur.execute("""INSERT INTO customers (external_id, cust_type)
    SELECT unique_id, 'Cust1' FROM cust1_demographics;""")
    
    cur.execute("""INSERT INTO customers (external_id, cust_type)
    SELECT unique_id, 'Cust2' FROM cust2_demographics;""")
    
    cur.execute("SELECT customer_id, external_id, cust_type FROM walmart.customers")
    lookup = {(ext_id, cust_type): cust_id for cust_id, ext_id, cust_type in cur.fetchall()}
    
    conn.commit()
    cur.close()
    
    return lookup
    


def load_all_with_customer_lookup(conn, tables):
    # conn = connect_to_db()
    cur = conn.cursor()
    cur.execute("SET search_path TO walmart;")
    
    #Load demographics and products data 
    demographic_tables = tables[:3]
    for table in demographic_tables:
        truncate_load_csv_to_table(conn, f'/Users/macos/Personal_projects/Portfolio/Project_1_Walmart/Walmart_sim/data_source/agm_output/{table}.csv', table)
    
    lookup = load_customer_lookup(conn)

    #Loading transactions data
    transaction_df = pd.read_csv('/Users/macos/Personal_projects/Portfolio/Project_1_Walmart/Walmart_sim/data_source/agm_output/transactions.csv')
    transaction_df['unique_id'] = transaction_df.apply( lambda row: lookup[(row['unique_id'], row['cust_type'])], axis=1)
    transaction_df.drop(columns = ['cust_type'], inplace = True)
    temp_path = '/tmp/transactions_modified.csv'
    transaction_df.to_csv(temp_path, index=False)
    truncate_load_csv_to_table(conn, temp_path, 'transactions')
    
    conn.commit()
    cur.close()



def main():
    # Connect to database
    try:
        tables = ['cust1_demographics', 'cust2_demographics', 'products', 'transactions', 'customers']
        
        print('Connecting to the database...')
        with connect_to_db() as conn:
            print("Connected to database")
            conn = connect_to_db()
            
            print("Setting up database schema...")
            conn = setup_database()

            load_all_with_customer_lookup(conn, tables)
            print("Data loading completed successfully!")
            
            # Add verification
            verify_tables(tables, conn)
            
    finally:
        conn.close()

if __name__ == "__main__":
    main()
