import pandas as pd
import psycopg2
from psycopg2 import sql
from dotenv import load_dotenv
from pathlib import Path
import os
import sys
from sqlalchemy import create_engine


"""
Loading csv files generated by AGM into Postgres
Methods:
- Connect to the database using variables in .envs
- Create the database schema using schema.sql
- Load the csv files into the database
    - use mogrify() for efficient batch insert


"""


# Load environment variables
load_dotenv()


def connect_to_db():
    """Establish connection to PostgreSQL database."""
    try:
        conn = psycopg2.connect(
            host=str(os.getenv('DB_HOST')),
            dbname=str(os.getenv('DB_NAME')),
            user=str(os.getenv('DB_USER')),
            password=str(os.getenv('DB_PASSWORD')),
            port=str(os.getenv('DB_PORT'))
        )
        return conn
    except Exception as e:
        print(f"Error connecting to database: {e}")
        raise

def setup_database():
    """Create database schema from SQL file."""
    try:
        conn = connect_to_db()
        cur = conn.cursor()
        
        # Read schema file
        with open('backend/database/schema.sql', 'r') as f:
            schema_sql = f.read()
        
        # Execute schema SQL
        cur.execute(schema_sql)
        conn.commit()
        
        # Verify schema was created
        cur.execute("SELECT schema_name FROM information_schema.schemata WHERE schema_name = 'walmart';")
        if cur.fetchone():
            print("Schema 'walmart' exists")
        else:
            print("Warning: Schema 'walmart' was not created")
            
        cur.close()
        conn.close()
    except Exception as e:
        print(f"Failed to create schema: {e}")  
    
def verify_tables(tables, conn):
    """Verify tables exist and show their row counts."""
    
    cur = conn.cursor()
    
    print("\nTable Verification:")
    print("-" * 50)
    
    for table in tables:
        # Check if table exists
        cur.execute("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'walmart' 
                AND table_name = %s
            );
        """, (table,))
        exists = cur.fetchone()[0]
        
        if exists == True:
            # Get row count
            cur.execute(f"SELECT COUNT(*) FROM walmart.{table}")
            count = cur.fetchone()[0]
            print(f"✓ {table}: {count} rows")
        else:
            print(f"✗ {table}: Table not found")
    
    cur.close()

def load_csv_to_table(conn, csv_path, table_name):
    """Load data from CSV file to PostgreSQL table."""
    print(f'Loading data for {table_name}')
    
    try:
        # Read CSV file
        df = pd.read_csv(csv_path)
        
        # Convert date strings to datetime objects
        if 'date_purchased' in df.columns:
            df['date_purchased'] = pd.to_datetime(df['date_purchased'])
        
        # Get the table schema columns to ensure correct order
        cur = conn.cursor()
        cur.execute(f"""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_schema = 'walmart' 
            AND table_name = '{table_name}'
            AND column_name NOT IN ('created_at', 'updated_at')
            ORDER BY ordinal_position;
        """)
        schema_columns = [col[0] for col in cur.fetchall()]
        
        # Reorder and select only columns that exist in schema
        df = df[schema_columns]
        
        # Dynamically setting the number of columns
        columns_str = ', '.join(schema_columns)
        placeholders_str = '(' + ', '.join(['%s'] * len(schema_columns)) + ')'  

        
        batch_size = len(df) // 2
        for i in range(0, len(df), batch_size):
            batch = df.iloc[i:i+batch_size].itertuples(index=False, name=None)
            args_str = b','.join(
                cur.mogrify(placeholders_str, row) for row in batch).decode('utf-8')
            
            # Add schema prefix to table name
            insert_query = f"INSERT INTO {table_name} ({columns_str}) VALUES {args_str}"
            cur.execute(insert_query)
            conn.commit()
        
        cur.close()
        print(f"Successfully loaded data from {csv_path} to {table_name}")
    except Exception as e:
        print(f"Failed to load data from {csv_path} to {table_name}: {e}")
        raise
    

def load_all_with_customer_lookup(tables):
    conn = connect_to_db()
    cur = conn.cursor()
    
    cur.execute("SET search_path TO walmart;")
    
    #Load demographics and products data 
    demographic_tables = tables[:3]
    for table in demographic_tables:
        cur.execute(f"TRUNCATE TABLE {table};")
        load_csv_to_table(conn, f'/Users/macos/Personal_projects/Portfolio/Project_1_Walmart/Walmart_sim/data_source/agm_output/{table}.csv', table)

    #Populate customers lookup table with data from cust1 and cust2
    cur.execute("""INSERT INTO customers (external_id, cust_type)
    SELECT unique_id, 'Cust1' FROM cust1_demographics;""")
    
    cur.execute("""INSERT INTO customers (external_id, cust_type)
    SELECT unique_id, 'Cust2' FROM cust2_demographics;""")
    
    cur.execute("SELECT customer_id, external_id, cust_type FROM walmart.customers")
    lookup = {(ext_id, cust_type): cust_id for cust_id, ext_id, cust_type in cur.fetchall()}

    #Loading transactions data
    transaction_df = pd.read_csv('/Users/macos/Personal_projects/Portfolio/Project_1_Walmart/Walmart_sim/data_source/agm_output/transactions.csv')
    transaction_df['unique_id'] = transaction_df.apply( lambda row: lookup[(row['unique_id'], row['customer_type'])], axis=1)
    transaction_df.drop(columns = ['customer_type'], inplace = True)
    load_csv_to_table(conn, transaction_df, 'transactions')
    conn.commit()
    cur.close()
    conn.close()



def main():
    # Connect to database
    try:
        tables = ['cust1_demographics', 'cust2_demographics', 'products', 'transactions', 'customers']
        
        print('Connecting to the database...')
        with connect_to_db() as conn:
            print("Connected to database")
            conn = connect_to_db()
            
            print("Setting up database schema...")
            setup_database()

            # Add verification
            verify_tables(tables, conn)
            
        
            load_all_with_customer_lookup(tables)
            print("Data loading completed successfully!")
            
    finally:
        conn.close()

if __name__ == "__main__":
    main()
