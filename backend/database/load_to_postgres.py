import io
import os
from pathlib import Path

import pandas as pd
import psycopg2
from dotenv import load_dotenv
from psycopg2 import sql

"""
Loading csv files generated by AGM into Postgres
Methods:
- Connect to the database using variables in .envs
- Create the database schema using schema.sql
- Load the csv files into the database
    - use mogrify() for efficient batch insert

Root directory: ./backend

Past Errors:
- Duplicate keys when inserting => duplicated keys when generating new runs => 
- Wrong (absolute) file path => turn into absolute
"""


# Always execute at data_pipeline directory
ROOT = Path(__file__).resolve().parent.parent
os.chdir(ROOT)
# Load environment variables
load_dotenv(ROOT / ".env")

# Global table names (should be consistent with schema tables)
tables = [
    "cust1",
    "cust2",
    "products",
    "transactions",
]

if "airflow" in str(ROOT):
    result_file_path = ROOT / Path("../data_source/agm_output")
else:
    result_file_path = ROOT / Path("../data_pipeline/data_source/agm_output")


def connect_to_db():
    """Establish connection to PostgreSQL database."""
    try:
        conn = psycopg2.connect(
            host=str(os.getenv("DB_HOST")),
            dbname=str(os.getenv("DB_NAME")),
            user=str(os.getenv("DB_USER")),
            password=str(os.getenv("DB_PASSWORD")),
            port=int(os.getenv("DB_PORT", 5432)),
        )
        return conn
    except Exception as e:
        print(f"Error connecting to database: {e}")
        raise


def setup_database():
    """Create database schema from SQL file."""

    conn = connect_to_db()
    cur = conn.cursor()

    with open("./database/schema.sql", "r") as f:
        schema_sql = f.read()
        cur.execute(schema_sql)
        conn.commit()

    print("Schema 'walmart' created successfully")

    cur.close()
    return conn  # Return the connection for further use


def verify_tables(tables, schema, conn):
    """
    Verify tables exist and show their row counts.
    Input:
    - tables: list of table names
    """

    cur = conn.cursor()

    print("\nTable Verification:")
    print("-" * 50)

    for table in tables:
        # Check if table exists
        cur.execute(
            f"""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = '{schema}'
                AND table_name = %s
            );
        """,
            (table,),
        )
        exists = cur.fetchone()[0]

        if exists:
            # Get row count
            cur.execute(f"SELECT COUNT(*) FROM walmart.{table}")
            count = cur.fetchone()[0]
            print(f"✓ {table}: {count} rows")
        else:
            print(f"✗ {table}: Table not found")

    cur.close()


def get_latest_result_folder(output_folder=result_file_path):
    data_folder = Path(output_folder)
    subfolder = [f for f in data_folder.iterdir()]
    latest_folder = max(subfolder, key=lambda x: str(x).split("=")[-1])
    return str(latest_folder)


def get_latest_file_paths(tables=tables):
    """
    Output: {"cust1":"../data_pipeline/data_source/agm_output/run_time=<time>/id=<id>_cust1.csv",...}
    """
    latest_result_folder = get_latest_result_folder()
    csv_path_dict = {}
    for t in tables:
        file_path = [str(x) for x in Path(latest_result_folder).glob(f"*{t}.csv")][0]
        csv_path_dict[t] = file_path
    return csv_path_dict


def get_target_schema_columns(cur, schema, table):
    """
    Select the columns from csv files that is needed to load into sql schema.
    Returns: columns in the schema table
    """
    cur.execute(
        """
        SELECT column_name
        FROM information_schema.columns
        WHERE table_schema=%s AND table_name=%s
        ORDER BY ordinal_position
    """,
        (schema, table),
    )
    return [r[0] for r in cur.fetchall()]


def stage_copy_dataframe(cur, schema, table, df, include_pk=False):
    """
    Create a TEMP staging table shaped like the target and COPY the dataframe into it.
    Input:
        - schema: sql schema (walmart)
        - table: table name (cust1, cust2,...)
        - df: dataframe from the csv
    Returns: (staging_table_name, load_cols)
    """
    # Normalize column names to lowercase to match Postgres conventions
    df.columns = [c.lower() for c in df.columns]
    exclude = {"created_at", "updated_at"}
    if not include_pk:
        exclude |= {"transaction_id"}

    target_cols = get_target_schema_columns(cur, schema, table)
    load_cols = [
        c for c in target_cols if c in df.columns and c not in exclude
    ]  # keep only existing columns

    staging = f"stg_{table}"
    cur.execute(
        sql.SQL("CREATE TEMP TABLE {} (LIKE {}.{} INCLUDING ALL);").format(
            sql.Identifier(staging), sql.Identifier(schema), sql.Identifier(table)
        )
    )

    # COPY from a CSV buffer
    buf = io.StringIO()  # A file placeholder
    df[load_cols].to_csv(buf, index=False)
    buf.seek(0)
    copy_sql = sql.SQL("COPY {} ({}) FROM STDIN WITH (FORMAT csv, HEADER true)").format(
        sql.Identifier(staging), sql.SQL(", ").join(map(sql.Identifier, load_cols))
    )
    cur.copy_expert(copy_sql.as_string(cur), buf)  # Run the copy command

    return staging, load_cols


def upsert_cust(cur, schema, cust_csv_paths):
    """
    Load and upsert customer dimension tables for both cust1 and cust2.
    Expects:
      cust_csv_paths = {"cust1": "path/to/cust1.csv", "cust2": "path/to/cust2.csv"}
    For each:
      PK = unique_id
      Upsert on unique_id; keep latest attributes; update run_id.
      Maintains customers_lookup table (idempotent).
    """
    for table in ("cust1", "cust2"):
        csv_path = cust_csv_paths.get(table)
        if not csv_path or not os.path.exists(csv_path):
            print(f"⚠ Skipping {table} — no CSV found.")
            continue

        df = pd.read_csv(csv_path)
        staging, cols = stage_copy_dataframe(
            cur, schema, table, df
        )  # Creating temporary staging tables
        key = "unique_id"
        set_expr = ", ".join([f"{c}=EXCLUDED.{c}" for c in cols if c != key])

        cur.execute(
            f"""
            INSERT INTO {schema}.{table} ({", ".join(cols)})
            SELECT {", ".join([f"s.{c}" for c in cols])}
            FROM {staging} s
            ON CONFLICT ({key}) DO UPDATE SET {set_expr};
        """
        )


def upsert_products(cur, schema, csv_paths):
    """
    products:
      PK = product_id
      Upsert on product_id; update attributes + run_id.
    """
    table = "products"
    product_paths = csv_paths.get(table)
    product_df = pd.read_csv(product_paths)

    staging, cols = stage_copy_dataframe(cur, schema, table, product_df)
    key = "product_id"
    set_expr = ", ".join([f"{c}=EXCLUDED.{c}" for c in cols if c != key])
    cur.execute(
        f"""
        INSERT INTO {schema}.{table} ({", ".join(cols)})
        SELECT {", ".join([f"s.{c}" for c in cols])}
        FROM {staging} s
        ON CONFLICT ({key}) DO UPDATE SET {set_expr};
    """
    )


def load_customer_lookup(cur, schema):
    cur.execute(
        f"""
    INSERT INTO {schema}.customers_lookup (cust1_id, segment_id, run_id)
    SELECT c.unique_id, c.segment_id, c.run_id
    FROM {schema}.cust1 AS c
    ON CONFLICT (cust1_id) DO UPDATE 
    SET segment_id = EXCLUDED.segment_id, run_id = EXCLUDED.run_id;
    """
    )

    cur.execute(
        f"""
    INSERT INTO {schema}.customers_lookup (cust2_id, segment_id, run_id)
    SELECT c.unique_id, c.segment_id, c.run_id
    FROM {schema}.cust2 AS c
    ON CONFLICT (cust2_id) DO UPDATE 
    SET segment_id = EXCLUDED.segment_id, run_id = EXCLUDED.run_id;
    """
    )


def load_transactions(cur, schema, csv_paths):
    """
    Transactions are immutable events so only append no update based on primary key -> on conflict do nothing
    Also, PK might not be needed but we added here for simplicity and consistency with other tables.
    """
    table = "transactions"
    trans_path = csv_paths.get(table)
    trans_df = pd.read_csv(trans_path)

    staging, cols = stage_copy_dataframe(cur, schema, table, trans_df, include_pk=True)
    # Simple idempotent insert:
    cur.execute(
        f"""
        INSERT INTO {schema}.{table} ({", ".join(cols)})
        SELECT 
        s.transaction_id,
        l.customer_id,
        s.product_id,
        s.unit_price,
        s.quantity,
        TO_DATE(s.date_purchased::text, 'YYYYMMDD'),
        s.category,
        s.run_id
        FROM {staging} s
        JOIN {schema}.customers_lookup l
        ON l.cust1_id = s.unique_id
        ON CONFLICT (transaction_id) DO NOTHING;
    """
    )

    cur.execute(
        f"""
        INSERT INTO {schema}.{table} ({", ".join(cols)})
        SELECT 
        s.transaction_id,
        l.customer_id,
        s.product_id,
        s.unit_price,
        s.quantity,
        TO_DATE(s.date_purchased::text, 'YYYYMMDD'),
        s.category,
        s.run_id
        FROM {staging} s
        JOIN {schema}.customers_lookup l
        ON l.cust2_id = s.unique_id
        ON CONFLICT (transaction_id) DO NOTHING;
    """
    )
    # Keep the SERIAL's sequence in sync with your explicit IDs
    cur.execute(
        f"""
        SELECT setval(
            pg_get_serial_sequence('{schema}.{table}', 'transaction_id'),
            COALESCE((SELECT MAX(transaction_id) FROM {schema}.transactions), 0)
        );
    """
    )


def main():
    # Connect to database
    schema_name = "walmart"
    print("Connecting to the database...")
    with connect_to_db() as conn:
        print("Connected to database")
        conn = connect_to_db()

        print("Setting up database schema...")
        conn = setup_database()

        cur = conn.cursor()
        file_paths = get_latest_file_paths()
        print("Loading customers...")
        upsert_cust(cur, schema_name, file_paths)
        print("Loading products...")
        upsert_products(cur, schema_name, file_paths)
        print("loading cust lookup table")
        load_customer_lookup(cur, schema_name)
        print("Loading transactions...")
        load_transactions(cur, schema_name, file_paths)

        cur.close()
        conn.commit()
        # Add verification
        verify_tables(tables, schema_name, conn)
    conn.close()


if __name__ == "__main__":
    main()
