import os
from pathlib import Path

import pandas as pd
import psycopg2
from dotenv import load_dotenv

"""
Loading csv files generated by AGM into Postgres
Methods:
- Connect to the database using variables in .envs
- Create the database schema using schema.sql
- Load the csv files into the database
    - use mogrify() for efficient batch insert

Root directory: ./backend

Past Errors:
- Duplicate keys when inserting => duplicated keys when generating new runs => 
- Wrong (absolute) file path => turn into absolute
"""

# Load environment variables
load_dotenv()

# Always execute at data_pipeline directory
ROOT = Path(__file__).resolve().parent.parent
os.chdir(ROOT)
print(os.getcwd())


def connect_to_db():
    """Establish connection to PostgreSQL database."""
    try:
        conn = psycopg2.connect(
            host=str(os.getenv("DB_HOST")),
            dbname=str(os.getenv("DB_NAME")),
            user=str(os.getenv("DB_USER")),
            password=str(os.getenv("DB_PASSWORD")),
            port=str(os.getenv("DB_PORT")),
        )
        return conn
    except Exception as e:
        print(f"Error connecting to database: {e}")
        raise


def setup_database():
    """Create database schema from SQL file."""

    conn = connect_to_db()
    cur = conn.cursor()

    with open("./database/schema.sql", "r") as f:
        schema_sql = f.read()
        cur.execute(schema_sql)
        conn.commit()

    print("Schema 'walmart' created successfully")

    cur.close()
    return conn  # Return the connection for further use


def verify_tables(tables, conn):
    """Verify tables exist and show their row counts."""

    cur = conn.cursor()

    print("\nTable Verification:")
    print("-" * 50)

    for table in tables:
        # Check if table exists
        cur.execute(
            """
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'walmart' 
                AND table_name = %s
            );
        """,
            (table,),
        )
        exists = cur.fetchone()[0]

        if exists:
            # Get row count
            cur.execute(f"SELECT COUNT(*) FROM walmart.{table}")
            count = cur.fetchone()[0]
            print(f"✓ {table}: {count} rows")
        else:
            print(f"✗ {table}: Table not found")

    cur.close()


def load_customers_lookup(conn, table="customers_lookup", schema="walmart"):
    """Create a lookup table for customer IDs for transaction table PK."""
    cur = conn.cursor()

    cur.execute(f"TRUNCATE TABLE {schema}.{table} CASCADE;")

    # Populate customers lookup table with data from cust1 and cust2 (autogenerated PK)
    cur.execute(
        f"""INSERT INTO {schema}.{table} (external_id, cust_type, segment_id)
    SELECT unique_id, 'Cust1', segment_id FROM cust1;"""
    )

    cur.execute(
        f"""INSERT INTO {schema}.{table} (external_id, cust_type, segment_id)
    SELECT unique_id, 'Cust2', segment_id FROM cust2;"""
    )

    # This lookup is used to map to transaction id
    cur.execute(f"SELECT customer_id, external_id, cust_type FROM {schema}.{table}")
    lookup = {
        (ext_id, cust_type): (cust_id) for cust_id, ext_id, cust_type in cur.fetchall()
    }

    conn.commit()
    cur.close()

    return lookup


def truncate_load_csv_to_table(conn, csv_path, table_name, schema="walmart"):
    """
    Load data from CSV file to PostgreSQL table.
    Column names must line up with the table schema.
    Reference: load_all_with_customer_lookup()
    """
    print(f"Loading data for {table_name}")

    try:
        # Read CSV file
        df = pd.read_csv(csv_path)

        # Convert date strings to datetime objects
        if "date_purchased" in df.columns:
            df["date_purchased"] = pd.to_datetime(df["date_purchased"])

        # Checking for duplicates
        assert df.iloc[:, 0].is_unique, print("Duplicate primary keys")

        # Get the table schema columns to ensure correct order
        cur = conn.cursor()
        print(f"Removing all rows in {table_name} for loading...")
        cur.execute(f"TRUNCATE TABLE {schema}.{table_name} RESTART IDENTITY CASCADE;")
        cur.execute(
            f"""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_schema = '{schema}'
            AND table_name = '{table_name}'
            AND column_name NOT IN ('created_at', 'updated_at', 'transaction_id', 'customer_id')
            ORDER BY ordinal_position;
        """
        )
        schema_columns = [col[0] for col in cur.fetchall()]

        # Reorder and select only columns that exist in schema
        df.columns = df.columns.str.lower()
        df = df[schema_columns]

        # Dynamically setting the number of columns
        columns_str = ", ".join(schema_columns)
        placeholders_str = "(" + ", ".join(["%s"] * len(schema_columns)) + ")"

        # Checkin if the table is empty for insert
        cur.execute(f"select count(*) from {schema}.{table_name};")
        database_count = cur.fetchall()[0][0]
        assert int(database_count) == 0, print(
            f"Table is not empty with {database_count}"
        )

        batch_size = 5000
        for i in range(0, len(df), batch_size):
            batch = df.iloc[i : i + batch_size].itertuples(index=False, name=None)
            args_str = b",".join(
                cur.mogrify(placeholders_str, row) for row in batch
            ).decode("utf-8")
            if i == 0:
                print(
                    f"Insert string length: {len(args_str)} First few characters: {args_str[0:100]}"
                )

            # Add schema prefix to table name
            insert_query = (
                f"INSERT INTO {schema}.{table_name} ({columns_str}) VALUES {args_str}"
            )
            cur.execute(insert_query)

        conn.commit()

        cur.close()
        print(f"Successfully loaded data to {schema}.{table_name}")
    except Exception as e:
        conn.rollback()
        print(f"Failed to load data to {schema}.{table_name}: {e}")
        raise


def get_latest_output_folder():
    data_folder = Path("../data_pipeline/data_source/agm_output")
    subfolder = [f for f in data_folder.iterdir()]
    latest_folder = max(subfolder, key=lambda x: str(x).split("=")[-1])
    return latest_folder


def load_all_with_customer_lookup(conn, tables):
    # conn = connect_to_db()
    cur = conn.cursor()
    cur.execute("SET search_path TO walmart;")

    latest_folder = get_latest_output_folder()

    # Load demographics and products data
    demographic_tables = tables[:3]
    for table in demographic_tables:
        target_file_path = next(latest_folder.glob(f"*{table}*.csv"))
        truncate_load_csv_to_table(conn, target_file_path, table)

    lookup = load_customers_lookup(conn)

    # Formatting transactions data
    transaction_file_path = next(latest_folder.glob("*transaction*.csv"))
    transaction_df = pd.read_csv(transaction_file_path)
    transaction_df["unique_id"] = transaction_df.apply(
        lambda row: lookup[(row["unique_id"], row["cust_type"])], axis=1
    )
    transaction_df.drop(columns=["cust_type"], inplace=True)
    transaction_df["date_purchased"] = pd.to_datetime(transaction_df["date_purchased"])

    temp_path = "/tmp/transactions_modified.csv"
    transaction_df.to_csv(temp_path, index=False)

    truncate_load_csv_to_table(conn, temp_path, "transactions")

    conn.commit()
    cur.close()


def main():
    # Connect to database
    tables = [
        "cust1",
        "cust2",
        "products",
        "transactions",
        "customers_lookup",
    ]

    print("Connecting to the database...")
    with connect_to_db() as conn:
        print("Connected to database")
        conn = connect_to_db()

        print("Setting up database schema...")
        conn = setup_database()

        load_all_with_customer_lookup(conn, tables)
        print("Data loading completed successfully!")

        print("checking current tables...")
        cur = conn.cursor()
        cur.execute(
            """
                    SELECT table_schema, table_name
                    FROM information_schema.tables
                    WHERE table_schema NOT IN ('pg_catalog','information_schema')
                    ORDER BY table_schema, table_name;"""
        )
        print("\nCurrent tables:\n" + "-" * 50)
        for schema, table in cur.fetchall():
            print(f"{schema}: {table}")

        # Add verification
        verify_tables(tables, conn)
    conn.close()


if __name__ == "__main__":
    main()
